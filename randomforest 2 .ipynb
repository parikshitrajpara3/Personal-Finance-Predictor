{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "     ---------------------------------------- 11.0/11.0 MB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.14.1-cp311-cp311-win_amd64.whl (44.8 MB)\n",
      "     ---------------------------------------- 44.8/44.8 MB 3.9 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "     -------------------------------------- 301.8/301.8 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import  accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Text to numeric for further processing\n",
    "label_encoder = LabelEncoder()\n",
    "df['City Tier'] = label_encoder.fit_transform(df['City_Tier'])\n",
    "df = pd.get_dummies(df,columns = ['Occupation'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Boolean values are in the int format for further processing\n",
    "df[['Occupation_Retired','Occupation_Self_Employed', 'Occupation_Student']] = df[['Occupation_Retired','Occupation_Self_Employed', 'Occupation_Student']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping City_Tier Column as it has been encoded\n",
    "df= df.drop('City_Tier', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers in the dataset\n",
    "\n",
    "def remove_outliers(df):\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    columns_to_check = ['Income', 'Age', 'Dependents', 'Rent', 'Loan_Repayment', 'Insurance',\n",
    "                        'Groceries', 'Transport', 'Eating_Out', 'Entertainment', 'Utilities',\n",
    "                        'Healthcare', 'Education', 'Miscellaneous',\n",
    "                        'Savings',  'Potential_Savings_Groceries',\n",
    "                        'Potential_Savings_Transport', 'Potential_Savings_Eating_Out',\n",
    "                        'Potential_Savings_Entertainment', 'Potential_Savings_Utilities',\n",
    "                        'Potential_Savings_Healthcare', 'Potential_Savings_Education',\n",
    "                        'Potential_Savings_Miscellaneous']\n",
    "\n",
    "    for column in columns_to_check:\n",
    "        \n",
    "        Q1 = df_clean[column].quantile(0.25)\n",
    "        Q3 = df_clean[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        df_clean = df_clean[(df_clean[column] >= lower_bound) & (df_clean[column] <= upper_bound)]\n",
    "\n",
    "    \n",
    "    df_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    \n",
    "    print(f\"Number of rows in original dataframe: {len(df)}\")\n",
    "    print(f\"Number of rows in cleaned dataframe: {len(df_clean)}\")\n",
    "    print(f\"Number of rows removed: {len(df) - len(df_clean)}\")\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "df= remove_outliers(df)\n",
    "\n",
    "df.to_csv('file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any duplicates in my data\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "print(df_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('file.csv',index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from kneed import KneeLocator\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def find_optimal_clusters(df):\n",
    "    # Feature engineering for ratios\n",
    "    feature_data = pd.DataFrame({\n",
    "        'entertainment_ratio': (df['Eating_Out'] + df['Entertainment']) / df['Income'],\n",
    "        'family_ratio': (df['Groceries'] + df['Rent'] + df['Transport']) / df['Income'],\n",
    "        'savings_ratio': df['Savings'] / df['Income'],\n",
    "    })\n",
    "    \n",
    "    # Scaling features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_data)\n",
    "    \n",
    "    # Dimensionality reduction with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_features = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # 1. Elbow Method\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    K = range(2, 11)\n",
    "    \n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(pca_features)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        if k > 1:  # Silhouette score requires at least 2 clusters\n",
    "            silhouette_scores.append(silhouette_score(pca_features, kmeans.labels_))\n",
    "    \n",
    "    # Plot Elbow Method\n",
    "    ax1 = fig.add_subplot(231)\n",
    "    ax1.plot(K, inertias, 'bx-')\n",
    "    ax1.set_xlabel('k')\n",
    "    ax1.set_ylabel('Inertia')\n",
    "    ax1.set_title('Elbow Method')\n",
    "    \n",
    "    # Find the elbow point\n",
    "    kl = KneeLocator(K, inertias, curve='convex', direction='decreasing')\n",
    "    ax1.axvline(x=kl.elbow, color='r', linestyle='--')\n",
    "    \n",
    "    # 2. Silhouette Analysis\n",
    "    ax2 = fig.add_subplot(232)\n",
    "    ax2.plot(K[1:], silhouette_scores[:-1], 'rx-') \n",
    "    ax2.set_xlabel('k')\n",
    "    ax2.set_ylabel('Silhouette Score')\n",
    "    ax2.set_title('Silhouette Analysis')\n",
    "    \n",
    "    # 3. Hierarchical Clustering Dendrogram\n",
    "    ax3 = fig.add_subplot(233)\n",
    "    linkage_matrix = linkage(pca_features, method='ward')\n",
    "    dendrogram(linkage_matrix, truncate_mode='lastp', p=10)\n",
    "    ax3.set_title('Hierarchical Clustering Dendrogram')\n",
    "    \n",
    "    # 4. DBSCAN Optimal Epsilon\n",
    "    neighbors = NearestNeighbors(n_neighbors=2)\n",
    "    neighbors_fit = neighbors.fit(pca_features)\n",
    "    distances, indices = neighbors_fit.kneighbors(pca_features)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    \n",
    "    ax4 = fig.add_subplot(234)\n",
    "    ax4.plot(np.arange(len(distances)), distances)\n",
    "    ax4.set_xlabel('Points')\n",
    "    ax4.set_ylabel('Distance')\n",
    "    ax4.set_title('DBSCAN: k-distance graph')\n",
    "    \n",
    "    # 5. Visualization of best clustering\n",
    "    optimal_k = kl.elbow\n",
    "    kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans_optimal.fit_predict(pca_features)\n",
    "    \n",
    "    ax5 = fig.add_subplot(235)\n",
    "    scatter = ax5.scatter(pca_features[:, 0], pca_features[:, 1], \n",
    "                         c=cluster_labels, cmap='viridis')\n",
    "    ax5.set_title(f'Clustering with Optimal k={optimal_k}')\n",
    "    plt.colorbar(scatter, ax=ax5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print analysis results\n",
    "    print(\"\\nOptimal Number of Clusters Analysis:\")\n",
    "    print(f\"1. Elbow Method suggests {kl.elbow} clusters\")\n",
    "    print(f\"2. Best Silhouette Score: {max(silhouette_scores):.3f} \"\n",
    "          f\"at k={silhouette_scores.index(max(silhouette_scores)) + 2}\")\n",
    "    \n",
    "    # Calculate additional cluster validity metrics\n",
    "    optimal_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    optimal_labels = optimal_kmeans.fit_predict(pca_features)\n",
    "    \n",
    "    # Calculate cluster characteristics\n",
    "    for i in range(optimal_k):\n",
    "        cluster_points = pca_features[optimal_labels == i]\n",
    "        cluster_center = np.mean(cluster_points, axis=0)\n",
    "        cluster_std = np.std(cluster_points, axis=0)\n",
    "        cluster_size = len(cluster_points)\n",
    "        \n",
    "        print(f\"\\nCluster {i} Statistics:\")\n",
    "        print(f\"Size: {cluster_size} points ({cluster_size/len(pca_features)*100:.1f}%)\")\n",
    "        print(f\"Compactness (avg distance to center): \"\n",
    "              f\"{np.mean(np.linalg.norm(cluster_points - cluster_center, axis=1)):.3f}\")\n",
    "        print(f\"Standard deviation: {np.mean(cluster_std):.3f}\")\n",
    "    \n",
    "    return optimal_k, pca_features, optimal_labels\n",
    "\n",
    "# Run the analysis\n",
    "optimal_k, pca_features, optimal_labels = find_optimal_clusters(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at your results, let me analyze the findings:\n",
    "\n",
    "Cluster Count Recommendations:\n",
    "\n",
    "\n",
    "Elbow Method suggests 4 clusters\n",
    "Silhouette Score suggests 3 clusters (score of 0.338)\n",
    "The relatively low silhouette score (0.338) indicates some overlap between clusters\n",
    "\n",
    "\n",
    "Cluster Statistics Analysis:\n",
    "\n",
    "\n",
    "The clusters are fairly well-balanced in size (22-28% each)\n",
    "Very similar compactness metrics (0.759-0.808)\n",
    "Similar standard deviations (0.593-0.633)\n",
    "This uniformity suggests natural groupings in the data\n",
    "\n",
    "Let's optimize the clustering for 3 clusters since:\n",
    "\n",
    "It has the best silhouette score\n",
    "The 4-cluster solution shows very similar compactness metrics, suggesting it might be over-segmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_optimized_clusters(df):\n",
    "    # Feature engineering for ratios\n",
    "    feature_data = pd.DataFrame({\n",
    "        'entertainment_ratio': (df['Eating_Out'] + df['Entertainment']) / df['Income'],\n",
    "        'family_ratio': (df['Groceries'] + df['Transport']) / df['Income'],\n",
    "        'savings_ratio': df['Savings'] / df['Income'],\n",
    "    })\n",
    "     \n",
    "    # Scaling features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_data)\n",
    "    \n",
    "    # Dimensionality reduction with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_features = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    # Optimal clustering with k=3\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=20)\n",
    "    cluster_labels = kmeans.fit_predict(pca_features)\n",
    "    \n",
    "    # Calculate silhouette scores for each point\n",
    "    silhouette_vals = silhouette_samples(pca_features, cluster_labels)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # 1. Main clustering plot\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    scatter = ax1.scatter(pca_features[:, 0], pca_features[:, 1], \n",
    "                         c=cluster_labels, cmap='viridis')\n",
    "    \n",
    "    # Plot cluster centers\n",
    "    centers = kmeans.cluster_centers_\n",
    "    ax1.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', \n",
    "                s=200, linewidths=3, label='Centroids')\n",
    "    \n",
    "    ax1.set_title('Optimized 3-Cluster Solution')\n",
    "    ax1.legend()\n",
    "    plt.colorbar(scatter)\n",
    "    \n",
    "    # 2. Silhouette analysis plot\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    y_lower = 10\n",
    "    for i in range(3):\n",
    "        cluster_silhouette_vals = silhouette_vals[cluster_labels == i]\n",
    "        cluster_silhouette_vals.sort()\n",
    "        cluster_size = cluster_silhouette_vals.shape[0]\n",
    "        y_upper = y_lower + cluster_size\n",
    "        \n",
    "        color = plt.cm.viridis(float(i) / 3)\n",
    "        ax2.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                         0, cluster_silhouette_vals,\n",
    "                         facecolor=color, alpha=0.7)\n",
    "        \n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    ax2.set_title('Silhouette Analysis')\n",
    "    ax2.set_xlabel('Silhouette Coefficient')\n",
    "    ax2.axvline(x=np.mean(silhouette_vals), color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and print detailed cluster characteristics\n",
    "    original_features = feature_data.values\n",
    "    \n",
    "    print(\"\\nDetailed Cluster Analysis:\")\n",
    "    for i in range(3):\n",
    "        cluster_mask = cluster_labels == i\n",
    "        cluster_points = original_features[cluster_mask]\n",
    "        \n",
    "        print(f\"\\nCluster {i} Profile:\")\n",
    "        print(f\"Size: {np.sum(cluster_mask)} points ({np.sum(cluster_mask)/len(cluster_labels)*100:.1f}%)\")\n",
    "        print(\"\\nAverage Ratios:\")\n",
    "        print(f\"Entertainment: {np.mean(cluster_points[:, 0]):.3f}\")\n",
    "        print(f\"Family: {np.mean(cluster_points[:, 1]):.3f}\")\n",
    "        print(f\"Savings: {np.mean(cluster_points[:, 2]):.3f}\")\n",
    "        \n",
    "        print(\"\\nSpending Pattern:\")\n",
    "        primary_focus = np.argmax(np.mean(cluster_points, axis=0))\n",
    "        if primary_focus == 0:\n",
    "            pattern = \"Entertainment-focused\"\n",
    "        elif primary_focus == 1:\n",
    "            pattern = \"Family-focused\"\n",
    "        else:\n",
    "            pattern = \"Savings-focused\"\n",
    "        print(f\"Primary spending pattern: {pattern}\")\n",
    "    \n",
    "    return cluster_labels, pca_features\n",
    "\n",
    "# Run the analysis\n",
    "cluster_labels, pca_features = analyze_optimized_clusters(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear Segmentation:\n",
    "\n",
    "\n",
    "The three clusters show distinct spending patterns\n",
    "Major differentiation is in savings behavior\n",
    "Family spending is a secondary differentiator\n",
    "Entertainment spending is relatively consistent\n",
    "\n",
    "\n",
    "Behavioral Insights:\n",
    "\n",
    "\n",
    "Two distinct saving behaviors (high vs. moderate)\n",
    "Family spending inversely correlates with savings\n",
    "Entertainment spending is relatively stable across segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "clusters = [\"Family-Focused (C0)\", \"High Savers (C1)\", \"Balanced Savers (C2)\"]\n",
    "entertainment = [7.5, 5.8, 7.6]\n",
    "family = [20.2, 19.3, 17.5]\n",
    "savings = [17.5, 30.3, 29.1]\n",
    "\n",
    "# Set bar width\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(clusters))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bar1 = ax.bar(index - bar_width, entertainment, bar_width, label='Entertainment', color='#8884d8')\n",
    "bar2 = ax.bar(index, family, bar_width, label='Family', color='#82ca9d')\n",
    "bar3 = ax.bar(index + bar_width, savings, bar_width, label='Savings', color='#ffc658')\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Clusters')\n",
    "ax.set_ylabel('Percentage of Income')\n",
    "ax.set_title('Cluster Spending Patterns (% of Income)')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(clusters)\n",
    "ax.legend()\n",
    "\n",
    "# Adding percentage annotations\n",
    "def add_labels(bars):\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, yval + 0.2, f'{yval}%', ha='center', va='bottom')\n",
    "\n",
    "add_labels(bar1)\n",
    "add_labels(bar2)\n",
    "add_labels(bar3)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key Insights\n",
    "insights = [\n",
    "    \"Cluster sizes are very balanced (32-34% each)\",\n",
    "    \"Entertainment spending is consistently low across all clusters (5.8-7.6%)\",\n",
    "    \"Family spending shows moderate variation (17.5-20.2%)\",\n",
    "    \"Savings shows the highest variation between clusters (17.5-30.3%)\"\n",
    "]\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "for insight in insights:\n",
    "    print(f\"- {insight}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your data characteristics and the clustering results we've seen, let me help you choose the optimal algorithm:\n",
    "Comparison of Best Clustering AlgorithmsClick to open code\n",
    "Based on your data characteristics:\n",
    "\n",
    "K-means is the recommended choice because:\n",
    "\n",
    "Your clusters are fairly balanced (32-34%)\n",
    "The spending ratios show clear centroids\n",
    "The clusters have similar variances\n",
    "The data shows good spherical separation in the transformed space\n",
    "Computationally efficient for your dataset size\n",
    "\n",
    "\n",
    "Advantages of K-means for your case:\n",
    "\n",
    "Produces clean, interpretable clusters\n",
    "Handles the scale of your data well (thousands of points)\n",
    "Creates balanced clusters which match your data's natural distribution\n",
    "The centroids have clear business interpretation (spending patterns)\n",
    "\n",
    "Why not other algorithms:\n",
    "\n",
    "DBSCAN:\n",
    "\n",
    "Cluster Shape: The clusters you identified are well-separated and likely spherical or near-spherical, which K-means can handle effectively. DBSCAN is more suited to data with irregular shapes and varying densities, which doesn't seem to apply to your dataset.\n",
    "Density-Based Assumptions: Your clusters have relatively similar densities. DBSCAN works best when there are clusters with significant density differences (dense areas vs. sparse areas), which doesn’t appear to be the case here.\n",
    "\n",
    "\n",
    "Spectral Clustering:\n",
    "\n",
    "Computationally expensive for your dataset size\n",
    "Overkill since your clusters are well-separated\n",
    "\n",
    "\n",
    "Hierarchical Clustering:\n",
    "\n",
    "Too computationally intensive for your dataset size\n",
    "Doesn't add value given your clear cluster structure\n",
    "\n",
    "\n",
    "Gaussian Mixture Models:\n",
    "\n",
    "More complex than needed\n",
    "K-means gives similarly good results with simpler interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the correct conditions will be these: \n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=3,\n",
    "    init='k-means++',    # Use k-means++ initialization\n",
    "    n_init=20,           # Multiple initializations for stability\n",
    "    random_state=42,     # For reproducibility\n",
    "    max_iter=300         # Allow enough iterations for convergence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the clustering model\n",
    "def train_clustering_model(df):\n",
    "    # Feature engineering for ratios\n",
    "    feature_data = pd.DataFrame({\n",
    "        'entertainment_ratio': (df['Eating_Out'] + df['Entertainment']) / df['Income'],\n",
    "        'family_ratio': (df['Groceries'] + df['Transport']) / df['Income'],\n",
    "        'savings_ratio': df['Savings'] / df['Income'],\n",
    "    })\n",
    "    \n",
    "    # Scaling features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_data)\n",
    "    \n",
    "    # Dimensionality reduction with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_features = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    # Fit KMeans with 3 clusters\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=20)\n",
    "    kmeans.fit(pca_features)\n",
    "    \n",
    "    # Return the KMeans model, scaler, and PCA for later prediction\n",
    "    return kmeans, scaler, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation of the cluster formation\n",
    "\n",
    "\"High Savers\": If the savings percentage is higher than 25% and the family spending is also high (above 19%), this cluster is labeled as \"High Savers\".\n",
    "\n",
    "\"Balanced Savers\": If the savings percentage is higher than 25% but family spending is relatively moderate (below 19%), it’s labeled as \"Balanced Savers\".\n",
    "\n",
    "\"Family-Focused\": If the savings percentage is lower (below 25%) and family spending is relatively high, this cluster is considered \"Family-Focused\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to predict the cluster for a new user's input\n",
    "def predict_user_cluster(kmeans, scaler, pca, new_user_data):\n",
    "    # Feature engineering for the new user's input\n",
    "    feature_data = pd.DataFrame({\n",
    "        'entertainment_ratio': (new_user_data['Eating_Out'] + new_user_data['Entertainment']) / new_user_data['Income'],\n",
    "        'family_ratio': (new_user_data['Groceries'] + new_user_data['Transport']) / new_user_data['Income'],\n",
    "        'savings_ratio': new_user_data['Savings'] / new_user_data['Income'],\n",
    "    }, index=[0])  # Ensure it's a DataFrame with one row\n",
    "    \n",
    "    # Scale the features\n",
    "    scaled_features = scaler.transform(feature_data)\n",
    "    \n",
    "    # Transform using PCA\n",
    "    pca_features = pca.transform(scaled_features)\n",
    "    \n",
    "    # Predict the cluster\n",
    "    cluster_label = kmeans.predict(pca_features)[0]\n",
    "    \n",
    "    # Map the cluster number to a human-readable name\n",
    "    cluster_names = ['High Savers', 'Balanced Savers', 'Family Focused']\n",
    "    predicted_cluster = cluster_names[cluster_label]\n",
    "    \n",
    "    return predicted_cluster\n",
    "\n",
    "# Testing\n",
    "kmeans, scaler, pca = train_clustering_model(df)\n",
    "\n",
    "# New user's data to predict (example)\n",
    "new_user_data = {\n",
    "    'Eating_Out': 400,\n",
    "    'Entertainment': 150,\n",
    "    'Groceries': 500,\n",
    "    'Transport': 200,\n",
    "    'Savings': 800,\n",
    "    'Income': 3000\n",
    "}\n",
    "\n",
    "# Predict the cluster for the new user\n",
    "predicted_cluster = predict_user_cluster(kmeans, scaler, pca, new_user_data)\n",
    "print(f\"The new user belongs to the cluster: {predicted_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming you have already run the KMeans model and have these variables:\n",
    "# scaler, pca, kmeans\n",
    "\n",
    "# Manually define the cluster names based on KMeans output:\n",
    "cluster_names = {\n",
    "    0: \"Family Focused\",    # For cluster 0\n",
    "    1: \"High Savers\",       # For cluster 1\n",
    "    2: \"Balanced Savers\",   # For cluster 2\n",
    "}\n",
    "\n",
    "def predict_and_print_results(df, kmeans, scaler, pca, cluster_names, new_user):\n",
    "    # Features for savings prediction\n",
    "    features_savings = ['Income', 'Age', 'Dependents', 'Rent', 'Loan_Repayment', \n",
    "                        'Groceries', 'Transport', 'Eating_Out', 'Entertainment', \n",
    "                        'City Tier', 'Occupation_Retired', 'Occupation_Self_Employed', \n",
    "                        'Occupation_Student']\n",
    "      \n",
    "    # Train a Random Forest for savings prediction\n",
    "    rf_savings = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_savings.fit(df[features_savings], df['Savings'])\n",
    "    predicted_savings = rf_savings.predict(new_user[features_savings])[0]\n",
    "\n",
    "    # Predict potential expenses in various categories\n",
    "    savings_categories = [col for col in df.columns if col.startswith('Potential_Savings_')]\n",
    "    features_expenses = ['Income', 'Age', 'Dependents', 'City Tier',\n",
    "                         'Occupation_Retired', 'Occupation_Self_Employed',\n",
    "                         'Occupation_Student']\n",
    "    \n",
    "    expense_predictions = {}\n",
    "    for category in savings_categories:\n",
    "        rf_expense = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf_expense.fit(df[features_expenses], df[category])\n",
    "        expense_predictions[category] = rf_expense.predict(new_user[features_expenses])[0]\n",
    "\n",
    "    # Calculate ratios for KMeans prediction (entertainment_ratio, family_ratio, savings_ratio)\n",
    "    entertainment_ratio = (new_user['Eating_Out'].values[0] + new_user['Entertainment'].values[0]) / new_user['Income'].values[0]\n",
    "    family_ratio = (new_user['Groceries'].values[0] + new_user['Transport'].values[0]) / new_user['Income'].values[0]\n",
    "    savings_ratio = new_user['Savings'] / new_user['Income'].values[0]  \n",
    "\n",
    "    # Prepare the input for KMeans prediction\n",
    "    cluster_input = pd.DataFrame({\n",
    "        'entertainment_ratio': [entertainment_ratio],\n",
    "        'family_ratio': [family_ratio],\n",
    "        'savings_ratio': [savings_ratio],\n",
    "    })\n",
    "\n",
    "    # Scale the input for KMeans and apply PCA transformation\n",
    "    scaled_input = scaler.transform(cluster_input)\n",
    "    pca_input = pca.transform(scaled_input)\n",
    "\n",
    "    # Predict the cluster using KMeans\n",
    "    predicted_cluster = kmeans.predict(pca_input)[0]\n",
    "    predicted_category = cluster_names.get(predicted_cluster, \"Unknown Category\")\n",
    "\n",
    "    # Print the results\n",
    "    print(\"\\n=== Financial Prediction Results ===\")\n",
    "    print(\"\\n1. User Profile Analysis:\")\n",
    "    print(f\"Spending Profile Type: {predicted_category}\")\n",
    "\n",
    "    print(\"\\n2. Savings Prediction:\")\n",
    "    print(f\"Recommended Monthly Savings: {predicted_savings:,.2f} Rs\")\n",
    "\n",
    "    print(\"\\n3. Potential Monthly Savings by Category:\")\n",
    "    for category, amount in expense_predictions.items():\n",
    "        category_name = category.replace('Potential_Savings_', '').replace('_', ' ')\n",
    "        print(f\"{category_name:<15} {amount:,.2f} Rs\")\n",
    "\n",
    "    return {\n",
    "        'predicted_savings': predicted_savings,\n",
    "        'expense_predictions': expense_predictions,\n",
    "        'user_cluster': predicted_cluster,\n",
    "        'user_category': predicted_category,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    new_user = pd.DataFrame({\n",
    "        'Income': [4000],\n",
    "        'Age': [30],\n",
    "        'Dependents': [0],\n",
    "        'Rent': [750],\n",
    "        'Loan_Repayment': [1000],\n",
    "        'Groceries': [300],\n",
    "        'Transport': [100],\n",
    "        'Eating_Out': [100],\n",
    "        'Entertainment': [100],\n",
    "        'City Tier': [0],\n",
    "        'Occupation_Retired': [0],\n",
    "        'Occupation_Self_Employed': [1],\n",
    "        'Occupation_Student': [0],\n",
    "        'Savings': [750]  \n",
    "    })\n",
    "    \n",
    "   \n",
    "    results = predict_and_print_results(df, kmeans, scaler, pca, cluster_names, new_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def calculate_and_print_mae_scores(df):\n",
    "    # Features for savings prediction\n",
    "    features_savings = ['Income', 'Age', 'Dependents', 'Rent', 'Loan_Repayment', \n",
    "                        'Groceries', 'Transport', 'Eating_Out', 'Entertainment', \n",
    "                        'City Tier', 'Occupation_Retired', 'Occupation_Self_Employed', \n",
    "                        'Occupation_Student']\n",
    "    \n",
    "    # Features for expense predictions\n",
    "    features_expenses = ['Income', 'Age', 'Dependents', 'City Tier',\n",
    "                         'Occupation_Retired', 'Occupation_Self_Employed',\n",
    "                         'Occupation_Student']\n",
    "    \n",
    "    # Savings prediction MAE\n",
    "    X = df[features_savings]\n",
    "    y = df['Savings']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    rf_savings = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_savings.fit(X_train, y_train)\n",
    "    y_pred = rf_savings.predict(X_test)\n",
    "    mae_savings = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n=== Model Performance Evaluation ===\")\n",
    "    print(f\"MAE for Savings Prediction: ${mae_savings:.2f}\")\n",
    "    \n",
    "    # Expense predictions MAE\n",
    "    savings_categories = [col for col in df.columns if col.startswith('Potential_Savings_')]\n",
    "    for category in savings_categories:\n",
    "        X = df[features_expenses]\n",
    "        y = df[category]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        rf_expense = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf_expense.fit(X_train, y_train)\n",
    "        y_pred = rf_expense.predict(X_test)\n",
    "        mae_expense = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        category_name = category.replace('Potential_Savings_', '').replace('_', ' ')\n",
    "        print(f\"MAE for {category_name} Prediction: ${mae_expense:.2f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming df is your DataFrame with all the necessary data\n",
    "    calculate_and_print_mae_scores(df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Feature selection for savings prediction\n",
    "features_savings = ['Income', 'Age', 'Dependents', 'Rent', 'Loan_Repayment', \n",
    "                    'Groceries', 'Transport', 'Eating_Out', 'Entertainment', \n",
    "                    'City Tier', 'Occupation_Retired', 'Occupation_Self_Employed', \n",
    "                    'Occupation_Student']\n",
    "\n",
    "# Function to plot underfitting/overfitting analysis\n",
    "def plot_fitting_analysis(df):\n",
    "    X = df[features_savings]\n",
    "    y = df['Savings']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Range of number of trees (n_estimators) to evaluate model complexity\n",
    "    n_estimators_range = [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "    \n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    # Loop through different model complexities\n",
    "    for n_estimators in n_estimators_range:\n",
    "        rf_model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on training and testing data\n",
    "        y_train_pred = rf_model.predict(X_train)\n",
    "        y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "        # Calculate Root Mean Squared Error (RMSE)\n",
    "        train_error = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        test_error = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "        # Store errors for plotting\n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "    # Plot the training and validation error\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(n_estimators_range, train_errors, label='Training Error', marker='o', color='blue')\n",
    "    plt.plot(n_estimators_range, test_errors, label='Validation Error', marker='s', color='red')\n",
    "    \n",
    "    # Highlight regions of underfitting, good fit, and overfitting\n",
    "    plt.axvline(x=5, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axvline(x=100, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.text(3, max(train_errors), 'Underfitting\\nRegion', ha='right')\n",
    "    plt.text(50, max(train_errors), 'Good\\nFit', ha='center')\n",
    "    plt.text(350, max(train_errors), 'Overfitting\\nRegion', ha='left')\n",
    "\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Number of Trees (n_estimators)')\n",
    "    plt.ylabel('Root Mean Squared Error (RMSE)')\n",
    "    plt.title('Model Complexity vs. Error\\n(Underfitting-Overfitting Analysis)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_fitting_analysis(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph illustrates the concept of underfitting and overfitting in machine learning, specifically showing how model complexity (measured by the number of trees/estimators) affects both training and validation error.\n",
    "Let's break it down:\n",
    "\n",
    "Axes:\n",
    "\n",
    "\n",
    "X-axis: Number of trees (n_estimators) on a logarithmic scale from 10⁰ to 10² (1 to 100 trees)\n",
    "Y-axis: Root Mean Square Error (RMSE) - lower values indicate better performance\n",
    "\n",
    "\n",
    "Two Lines:\n",
    "\n",
    "\n",
    "Blue line: Training Error\n",
    "Red line: Validation Error\n",
    "\n",
    "\n",
    "Three Regions:\n",
    "\n",
    "\n",
    "Underfitting Region (left): High error on both training and validation sets\n",
    "Good Fit (middle): Optimal balance between training and validation error\n",
    "Overfitting Region (right): Low training error but validation error plateaus\n",
    "\n",
    "\n",
    "Key Observations:\n",
    "\n",
    "\n",
    "As more trees are added, both errors initially decrease\n",
    "Training error consistently decreases\n",
    "Validation error eventually plateaus\n",
    "The optimal model complexity appears to be around where the validation error levels off\n",
    "\n",
    "\n",
    "Best Practice:\n",
    "\n",
    "\n",
    "Choose the number of trees where validation error stabilizes (around 10-20 trees in this case)\n",
    "Adding more trees beyond this point doesn't significantly improve model performance\n",
    "This helps avoid both underfitting (too simple) and overfitting (too complex)\n",
    "\n",
    "This is a classic example of the bias-variance tradeoff in machine learning, showing how model complexity needs to be balanced to achieve optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the models for Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Function to train and save the models\n",
    "def train_and_save_models(df):\n",
    "    # Feature engineering for ratios\n",
    "    feature_data = pd.DataFrame({\n",
    "        'entertainment_ratio': (df['Eating_Out'] + df['Entertainment']) / df['Income'],\n",
    "        'family_ratio': (df['Groceries'] + df['Transport']) / df['Income'],\n",
    "        'savings_ratio': df['Savings'] / df['Income'],\n",
    "    })\n",
    "    \n",
    "    # Scaling features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_data)\n",
    "    \n",
    "    # Dimensionality reduction with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_features = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    # Fit KMeans with 3 clusters\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=20)\n",
    "    kmeans.fit(pca_features)\n",
    "    \n",
    "    # Save models and preprocessors\n",
    "    joblib.dump(kmeans, 'kmeans_model.pkl')\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "    joblib.dump(pca, 'pca_model.pkl')\n",
    "\n",
    "    # Train and save Random Forest model for savings prediction\n",
    "    features_savings = ['Income', 'Age', 'Dependents', 'Rent', 'Loan_Repayment', 'Groceries', 'Transport', \n",
    "                        'Eating_Out', 'Entertainment', 'City Tier', 'Occupation_Retired', \n",
    "                        'Occupation_Self_Employed', 'Occupation_Student']\n",
    "    rf_savings = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_savings.fit(df[features_savings], df['Savings'])\n",
    "    joblib.dump(rf_savings, 'rf_savings_model.pkl')\n",
    "\n",
    "    # Train and save Random Forest models for expense prediction\n",
    "    savings_categories = [col for col in df.columns if col.startswith('Potential_Savings_')]\n",
    "    rf_expense_models = {}\n",
    "    for category in savings_categories:\n",
    "        rf_expense = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf_expense.fit(df[features_savings], df[category])\n",
    "        rf_expense_models[category] = rf_expense\n",
    "        joblib.dump(rf_expense, f'{category}_model.pkl')\n",
    "\n",
    "    print(\"Models have been saved.\")\n",
    "\n",
    "# Example DataFrame df (you can replace this with your actual data)\n",
    "df = pd.read_csv(\"file.csv\")\n",
    "train_and_save_models(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
